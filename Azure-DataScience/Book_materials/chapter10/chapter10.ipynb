{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Chapter 10 code snippets\n",
        "This notebook contains all code snippets from chapter 10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1623173177123
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Upgrade to latest the packages used in this notebook\n",
        "!pip install --upgrade interpret-community\n",
        "!pip install --upgrade raiwidgets\n",
        "!pip install --upgrade fairlearn\n",
        "raise Exception(\"Please comment out this cell and restart the Jupyter kernel\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Interpreting the predictions of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1628358560043
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_classification\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "features, target = make_classification(\n",
        "    n_samples=500, n_features=3,\n",
        "    n_redundant=1, shift=0, scale=1,\n",
        "    weights=[0.7, 0.3], random_state=1337)\n",
        "\n",
        "def fix_series(series, min_val, max_val):\n",
        "    series = series - min(series)\n",
        "    series = series / max(series)\n",
        "    series = series * (max_val - min_val) + min_val\n",
        "    return series.round(0)\n",
        "\n",
        "features[:,0] = fix_series(features[:,0], 0, 10000)\n",
        "features[:,1] = fix_series(features[:,1], 0, 10)\n",
        "features[:,2] = fix_series(features[:,2], 18, 85)\n",
        "\n",
        "classsification_df = pd.DataFrame(features, dtype='int')\n",
        "classsification_df.set_axis([\n",
        "    'income','credit_cards', 'age'], \n",
        "    axis=1, inplace=True)\n",
        "\n",
        "classsification_df['approved_loan']= target\n",
        "classsification_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1628358571571
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core import Workspace, Dataset\n",
        "\n",
        "ws = Workspace.from_config()\n",
        "dstore = ws.get_default_datastore()\n",
        "loans_dataset = Dataset.Tabular.register_pandas_dataframe(\n",
        "    dataframe=classsification_df,\n",
        "    target=(dstore,\"/samples/loans\"),\n",
        "    name=\"loans\",\n",
        "    description=\"A genarated dataset for loans\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1628358571726
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x = classsification_df[['income','credit_cards', 'age']]\n",
        "y = classsification_df['approved_loan'].values\n",
        "x_train, x_test, y_train, y_test = \\\n",
        "        train_test_split(x, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1628358574744
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "datatransformer = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('scale', MinMaxScaler(), x_train.columns)\n",
        "])\n",
        "\n",
        "model_pipeline = Pipeline(steps=[\n",
        "                      ('datatransformer', datatransformer),\n",
        "                      ('model', RandomForestClassifier())\n",
        "])\n",
        "\n",
        "# Could be as simple as the following line\n",
        "# model_pipeline = RandomForestClassifier()\n",
        "model_pipeline.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1628358577899
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "test_df = pd.DataFrame(data=[\n",
        "    [2000, 2, 45],\n",
        "    [2000, 9, 45],\n",
        "    [10000, 2, 45]\n",
        "], columns=[\n",
        "    'income','credit_cards', 'age'\n",
        "])\n",
        "\n",
        "test_pred = model_pipeline.predict(test_df)\n",
        "print(test_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1628358579930
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "model_pipeline.named_steps['model'].feature_importances_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1628358584752
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from interpret.ext.blackbox import TabularExplainer\n",
        "\n",
        "explainer = TabularExplainer(\n",
        "                model_pipeline.named_steps['model'],\n",
        "                initialization_examples=x_train, \n",
        "                features= x_train.columns,\n",
        "                classes=[\"Reject\", \"Approve\"],\n",
        "                transformations=model_pipeline.named_steps['datatransformer'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1628358586386
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "local_explanation = explainer.explain_local(test_df)\n",
        "\n",
        "sorted_local_values = \\\n",
        "    local_explanation.get_ranked_local_values()\n",
        "sorted_local_names = \\\n",
        "    local_explanation.get_ranked_local_names()\n",
        "\n",
        "for sample_index in range(0,test_df.shape[0]):\n",
        "    print(f\"Test sample number {sample_index+1}\")\n",
        "    print(\"\\t\", test_df.iloc[[sample_index]]\n",
        "                         .to_dict(orient='list'))\n",
        "    prediction = test_pred[sample_index]\n",
        "    print(\"\\t\", f\"The prediction was {prediction}\")\n",
        "    importance_values = \\\n",
        "        sorted_local_values[prediction][sample_index]\n",
        "    importance_names = \\\n",
        "        sorted_local_names[prediction][sample_index]\n",
        "    local_importance = dict(zip(importance_names,\n",
        "                                importance_values))\n",
        "    print(\"\\t\", \"Local feature importance\")\n",
        "    print(\"\\t\", local_importance)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1628358589902
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "global_explanation = explainer.explain_global(x_test)\n",
        "print(\"Feature names:\", \n",
        "        global_explanation.get_ranked_global_names())\n",
        "print(\"Feature importances:\",\n",
        "        global_explanation.get_ranked_global_values())\n",
        "print(f\"Method used: {explainer._method}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1628357429582
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from raiwidgets import ExplanationDashboard\n",
        "ExplanationDashboard(global_explanation, model_pipeline, \n",
        "                      dataset=x_test, true_y=y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Understanding the tabular data interpretation techniques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1628357700035
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from interpret.ext.glassbox import (\n",
        "    LGBMExplainableModel,\n",
        "    LinearExplainableModel,\n",
        "    SGDExplainableModel,\n",
        "    DecisionTreeExplainableModel\n",
        ")\n",
        "\n",
        "from interpret.ext.blackbox import MimicExplainer\n",
        "mimic_explainer = MimicExplainer(\n",
        "                           model=model_pipeline, \n",
        "                           initialization_examples=x_train,\n",
        "                           explainable_model=DecisionTreeExplainableModel,\n",
        "                           augment_data=True, \n",
        "                           max_num_of_augmentations=10,\n",
        "                           features=x_train.columns,\n",
        "                           classes=[\"Reject\", \"Approve\"], \n",
        "                           model_task='classification')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1628357703669
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "mimic_global_explanation = \\\n",
        "        mimic_explainer.explain_global(x_test)\n",
        "print(\"Feature names:\", \n",
        "        mimic_global_explanation.get_ranked_global_names())\n",
        "print(\"Feature importances:\",\n",
        "        mimic_global_explanation.get_ranked_global_values())\n",
        "print(f\"Method used: {mimic_explainer._method}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1628357801377
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "mimic_local_explanation = mimic_explainer.explain_local(test_df)\n",
        "mimic_sorted_local_values = \\\n",
        "    mimic_local_explanation.get_ranked_local_values()\n",
        "mimic_sorted_local_names = \\\n",
        "    mimic_local_explanation.get_ranked_local_names()\n",
        "for sample_index in range(0,test_df.shape[0]):\n",
        "    print(f\"Test sample number {sample_index+1}\")\n",
        "    print(\"\\t\", test_df.iloc[[sample_index]]\n",
        "                         .to_dict(orient='list'))\n",
        "    prediction = test_pred[sample_index]\n",
        "    print(\"\\t\", f\"The prediction was {prediction}\")\n",
        "    mimic_importance_values = \\\n",
        "        mimic_sorted_local_values[prediction][sample_index]\n",
        "    mimic_importance_names = \\\n",
        "        mimic_sorted_local_names[prediction][sample_index]\n",
        "    mimic_local_importance = dict(zip(mimic_importance_names,\n",
        "                                mimic_importance_values))\n",
        "    print(\"\\t\", \"Local feature importance\")\n",
        "    print(\"\\t\", mimic_local_importance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1628357830491
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from interpret.ext.blackbox import PFIExplainer\n",
        "pfi_explainer = PFIExplainer(model_pipeline,\n",
        "                             features=x_train.columns,\n",
        "                             classes=[\"Reject\", \"Approve\"]\n",
        "                             )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1628357847887
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "pfi_global_explanation = \\\n",
        "        pfi_explainer.explain_global(x_test, \n",
        "                                     true_labels=y_test)\n",
        "print(\"Feature names:\", \n",
        "        pfi_global_explanation.get_ranked_global_names())\n",
        "print(\"Feature importances:\",\n",
        "        pfi_global_explanation.get_ranked_global_values())\n",
        "print(f\"Method used: {pfi_explainer._method}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Reviewing the interpretation results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1628358877310
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core import Workspace, Experiment\n",
        "from azureml.interpret import ExplanationClient\n",
        "\n",
        "ws = Workspace.from_config()\n",
        "exp = Experiment(workspace=ws, name=\"chapter10\")\n",
        "run = exp.start_logging(snapshot_directory=None)\n",
        "client = ExplanationClient.from_run(run)\n",
        "client.upload_model_explanation(\n",
        "    global_explanation, \n",
        "    true_ys= y_test,\n",
        "    comment='global explanation: TabularExplainer'\n",
        "\n",
        ")\n",
        "run.complete()\n",
        "print(run.get_portal_url())\n",
        "\n",
        "# If you get a ModuleNotFoundError: No module named 'shap.common', use the following code in a new cell to instal a specific version of shap:\n",
        "# !pip install --upgrade shap==0.34\n",
        "# If the problem persists, restart the kernel and start over the execution of all steps above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1628359840510
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core import Workspace, Dataset, Experiment\n",
        "from azureml.train.automl import AutoMLConfig\n",
        "\n",
        "ws = Workspace.from_config()\n",
        "compute_target = ws.compute_targets[\"cpu-sm-cluster\"]\n",
        "\n",
        "loans_dataset = Dataset.get_by_name(workspace=ws, name='loans')\n",
        "\n",
        "train_ds,validate_ds = loans_dataset.random_split(percentage=0.8, seed=1337)\n",
        "\n",
        "experiment_config = AutoMLConfig(\n",
        "    task = \"classification\",\n",
        "    primary_metric = 'accuracy',\n",
        "    training_data = train_ds,\n",
        "    label_column_name = \"approved_loan\",\n",
        "    validation_data = validate_ds,\n",
        "    compute_target = compute_target,\n",
        "    experiment_timeout_hours = 0.25,\n",
        "    iterations = 4,\n",
        "    model_explainability = True\n",
        ")\n",
        "\n",
        "automl_experiment = Experiment(ws, 'loans-automl')\n",
        "automl_run = automl_experiment.submit(experiment_config)\n",
        "\n",
        "automl_run.wait_for_completion(show_output=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Analyzing model errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1628365574416
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from raiwidgets import ErrorAnalysisDashboard\n",
        "ErrorAnalysisDashboard(global_explanation, model_pipeline, \n",
        "                       dataset=x_test, true_y=y_test)\n",
        "\n",
        "# If you don't see any graphics, open the notebook in Jupyter or Jupyter Lab\n",
        "# and re run the cell to generate the interactive widget."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Detecting potential model fairness issues\n",
        "You have already installed the fairlearn in the top of this notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1628370496143
        }
      },
      "outputs": [],
      "source": [
        "from fairlearn.metrics import MetricFrame\n",
        "from sklearn.metrics import accuracy_score\n",
        "y_pred = model_pipeline.predict(x_test)\n",
        "\n",
        "age = x_test['age']\n",
        "model_metrics = MetricFrame(accuracy_score, y_test, \n",
        "                             y_pred, sensitive_features=age)\n",
        "print(model_metrics.overall)\n",
        "print(model_metrics.by_group[model_metrics.by_group < 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1623174036016
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from raiwidgets import FairnessDashboard\n",
        "\n",
        "FairnessDashboard(\n",
        "    sensitive_features=age,\n",
        "    y_true=y_test,\n",
        "    y_pred=y_pred)\n",
        "\n",
        "# Open the notebook in Jupyter or Jupyter Lab to run this cell"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.6 - AzureML",
      "language": "python",
      "name": "python3-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
